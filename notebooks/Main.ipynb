{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e53a62a",
   "metadata": {},
   "source": [
    "# Main Pipeline -- EV Adoption Analysis (2015–2024)\n",
    "\n",
    "## Purpose\n",
    "This notebook orchestrates the **core data analytics pipeline** of the project.\n",
    "It loads the merged canton-level panel dataset and serves as the main entry point\n",
    "for descriptive analysis, preprocessing and model execution.\n",
    "\n",
    "## Key Steps\n",
    "- Load the cleaned canton-year panel dataset\n",
    "- Verify data integrity and structure\n",
    "- Prepare features for downstream modeling\n",
    "- Call modeling routines and aggregate results\n",
    "\n",
    "## Inputs\n",
    "- `data/intermediate/master_panel_2015_2024.csv`\n",
    "\n",
    "## Outputs\n",
    "- Model performance tables (R², RMSE, MAE)\n",
    "- Figures saved to `data/outputs/figures/`\n",
    "- Tables saved to `data/outputs/tables/`\n",
    "\n",
    "## Execution\n",
    "This notebook is executed automatically via `run_pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e483006c-c4ea-4748-8ebe-fae7a4c62ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: C:\\Users\\hamza\\OneDrive\\Desktop\\projet-ada-hk\n",
      "RAW : C:\\Users\\hamza\\OneDrive\\Desktop\\projet-ada-hk\\data\\raw\n",
      "RAW files: ['Bilan_pop_CH (old).xlsx', 'Bilan_pop_CH.xlsx', 'ElecProd_ByYear.xlsx', 'canton_climate_co2.xlsx', 'elcom', 'ev_registrations_per_canton.csv', 'gdp_per_capita.xlsx', 'motorisation_rate.xlsx', 'policy_parties_cantons.xlsx']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "cwd = Path.cwd().resolve()\n",
    "\n",
    "# case 1: launched from the root\n",
    "if (cwd / \"data\" / \"raw\").exists():\n",
    "    ROOT = cwd\n",
    "# case 2: launched from notebooks/\n",
    "elif (cwd.parent / \"data\" / \"raw\").exists():\n",
    "    ROOT = cwd.parent\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Project root not found from cwd={cwd}. \"\n",
    "        \"Expected to find data/raw in cwd or in its parent.\"\n",
    "    )\n",
    "\n",
    "DATA = ROOT / \"data\"\n",
    "RAW = DATA / \"raw\"\n",
    "ELCOM_RAW = RAW / \"elcom\"\n",
    "INTER = DATA / \"intermediate\"\n",
    "OUT = DATA / \"outputs\"\n",
    "\n",
    "INTER.mkdir(parents=True, exist_ok=True)\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"RAW :\", RAW)\n",
    "print(\"RAW files:\", sorted([p.name for p in RAW.iterdir()])[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OUTPUT FOLDERS (FIGURES + TABLES)\n",
    "# ============================================================\n",
    "from typing import Optional\n",
    "\n",
    "FIG_DIR = OUT / \"figures\"\n",
    "TAB_DIR = OUT / \"tables\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(filename: str, dpi: int = 200):\n",
    "    \"\"\"Save current matplotlib figure to Outputs/figures/\"\"\"\n",
    "    path = FIG_DIR / filename\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(f\" Figure saved: {path}\")\n",
    "    plt.close()\n",
    "\n",
    "def save_table(df, filename_csv: str, filename_xlsx: Optional[str] = None):\n",
    "    \"\"\"Save a DataFrame to Outputs/tables/ as CSV (+ optional XLSX).\"\"\"\n",
    "    csv_path = TAB_DIR / filename_csv\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\" Table saved: {csv_path}\")\n",
    "    if filename_xlsx:\n",
    "        xlsx_path = TAB_DIR / filename_xlsx\n",
    "        df.to_excel(xlsx_path, index=False)   \n",
    "        print(f\" Table saved: {xlsx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "confused-capability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the file path to the electricity production files used in our analysis, with only valid lines.\n",
      "- elecprod_byyear_clean_full.csv\n",
      "- elecprod_byyear_2015_2024.csv\n",
      "Years going from 1970 → 2024\n",
      "Total number of lines : 55\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "xlsx_path = RAW / \"ElecProd_ByYear.xlsx\"\n",
    "df = pd.read_excel(xlsx_path, dtype=str)\n",
    "\n",
    "df = df.rename(columns={\n",
    "    \"Year\": \"year\",\n",
    "    \"Production brute\": \"production_gross\",\n",
    "    \"Production nette\": \"production_net\"\n",
    "})\n",
    "years = []\n",
    "for val in df[\"year\"]:\n",
    "    val = str(val)\n",
    "    year = \"\".join([c for c in val if c.isdigit()])\n",
    "    if len(year) == 4:\n",
    "        years.append(int(year))\n",
    "    else:\n",
    "        years.append(None)\n",
    "df[\"year\"] = years\n",
    "df = df[[y is not None for y in df[\"year\"]]]  \n",
    "def to_float(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    x = str(x).replace(\" \", \"\").replace(\"\\xa0\", \"\").replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df[\"production_gross\"] = [to_float(x) for x in df[\"production_gross\"]]\n",
    "df[\"production_net\"] = [to_float(x) for x in df[\"production_net\"]]\n",
    "\n",
    "import csv\n",
    "df = df[pd.notna(df[\"year\"])].copy()\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "df = df.sort_values(\"year\").reset_index(drop=True)\n",
    "full_path = INTER / \"elecprod_byyear_clean_full.csv\"\n",
    "subset_path = INTER / \"elecprod_byyear_2015_2024.csv\"\n",
    "with open(full_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"year\", \"production_gross\", \"production_net\"])\n",
    "    for row in df.itertuples(index=False):\n",
    "        writer.writerow([row.year, row.production_gross, row.production_net])\n",
    "with open(subset_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"year\", \"production_gross\", \"production_net\"])\n",
    "    for row in df.itertuples(index=False):\n",
    "        try:\n",
    "            y = int(row.year)\n",
    "            if 2015 <= y <= 2024:\n",
    "                writer.writerow([row.year, row.production_gross, row.production_net])\n",
    "        except:\n",
    "            continue  \n",
    "\n",
    "print(\"Here are the file path to the electricity production files used in our analysis, with only valid lines.\")\n",
    "print(\"-\", full_path.name)\n",
    "print(\"-\", subset_path.name)\n",
    "print(\"Years going from\", df['year'].min(), \"→\", df['year'].max())\n",
    "print(\"Total number of lines :\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "advisory-emerald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ev_registrations_per_canton_clean_full.csv | ev_registrations_per_canton_2015_2024.csv\n",
      "We have 27 cantons and 540 rows.\n",
      "Years going from 2005 → 2024\n"
     ]
    }
   ],
   "source": [
    "csv_path = RAW / \"ev_registrations_per_canton.csv\"\n",
    "df = pd.read_csv(csv_path, dtype=str)\n",
    "df = df.rename(columns={\n",
    "    \"canton\": \"canton\",\n",
    "    \"year\": \"year\",\n",
    "    \"reg_total\": \"reg_total\",\n",
    "    \"ev_reg_count\": \"ev_reg_count\",\n",
    "    \"ev_reg_share\": \"ev_reg_share\",\n",
    "})\n",
    "\n",
    "# canton: trim + multiple spaces -> simple\n",
    "\n",
    "df[\"canton\"] = df[\"canton\"].astype(str).str.strip().str.replace(r\"\\s+\", \" \", regex=True)\n",
    "years = []\n",
    "for v in df[\"year\"]:\n",
    "    y = \"\".join(c for c in str(v) if c.isdigit())\n",
    "    years.append(int(y) if len(y) == 4 else None)\n",
    "df[\"year\"] = years\n",
    "df = df[[y is not None for y in df[\"year\"]]].copy()\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "def to_float(x):\n",
    "    x = \"\" if pd.isna(x) else str(x)\n",
    "    x = x.replace(\"\\xa0\", \"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for col in [\"reg_total\", \"ev_reg_count\", \"ev_reg_share\"]:\n",
    "    df[col] = [to_float(x) for x in df[col]]\n",
    "df = df[[\"canton\", \"year\", \"reg_total\", \"ev_reg_count\", \"ev_reg_share\"]].sort_values([\"canton\",\"year\"]).reset_index(drop=True)\n",
    "\n",
    "full_path   = INTER / \"ev_registrations_per_canton_clean_full.csv\"\n",
    "subset_path = INTER / \"ev_registrations_per_canton_2015_2024.csv\"\n",
    "\n",
    "with open(full_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow(df.columns)\n",
    "    for r in df.itertuples(index=False):\n",
    "        w.writerow([r.canton, r.year, r.reg_total, r.ev_reg_count, r.ev_reg_share])\n",
    "\n",
    "with open(subset_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow(df.columns)\n",
    "    for r in df.itertuples(index=False):\n",
    "        if 2015 <= r.year <= 2024:\n",
    "            w.writerow([r.canton, r.year, r.reg_total, r.ev_reg_count, r.ev_reg_share])\n",
    "\n",
    "print(\"Saved:\", full_path.name, \"|\", subset_path.name)\n",
    "print(\"We have\", df['canton'].nunique(), \"cantons and\", len(df), \"rows.\")\n",
    "print(\"Years going from\", df['year'].min(), \"→\", df['year'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-poster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Valid lines collected : 143402\n",
      "Files : elcom_prices_raw_minimal_2014_2025.csv | elcom_prices_by_operator_year_2014_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# ELCOM (from 2014 to 2025): column detection & cleaning\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv, re\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DATA_SRC = ELCOM_RAW\n",
    "DATA_INTER = ROOT / \"data\" / \"intermediate\"\n",
    "DATA_INTER.mkdir(parents=True, exist_ok=True)\n",
    "YEAR_MIN, YEAR_MAX = 2014, 2025\n",
    "\n",
    "def norm(s):\n",
    "    return re.sub(r\"\\s+\", \"\", str(s).strip().lower())\n",
    "\n",
    "def find_col(cols, patterns):\n",
    "    \"\"\"Returns the first column whose name matches one of the patterns (regex on normalised name).\"\"\"\n",
    "    ncols = {norm(c): c for c in cols}\n",
    "    for pat in patterns:\n",
    "        rx = re.compile(pat)\n",
    "        for n,c in ncols.items():\n",
    "            if rx.search(n):\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def find_price_col(cols):\n",
    "    # variations/misspellings of \"grid usage after discount (cts./kWh)\"\n",
    "    pats = [\n",
    "        r\"gridusage.*after.*discount.*cts.?/?.?kwh\",\n",
    "        r\"after.*discount.*cts.?/?.?kwh\",\n",
    "        r\"after.*discount\",  # fallback\n",
    "    ]\n",
    "    return find_col(cols, pats)\n",
    "\n",
    "def find_period_col(cols):\n",
    "    return find_col(cols, [r\"^period$\"])\n",
    "\n",
    "def find_operatorlabel_col(cols):\n",
    "    # \"operatorlabel\", \"operator label\", etc.\n",
    "    return find_col(cols, [r\"operator.*label\"])\n",
    "\n",
    "def find_gridusagename_col(cols):\n",
    "    # \"gridusagename\", \"grid usage name\", etc.\n",
    "    return find_col(cols, [r\"grid.*usage.*name\", r\"gridusagename\"])\n",
    "\n",
    "def to_float(x):\n",
    "    x = \"\" if pd.isna(x) else str(x)\n",
    "    x = x.replace(\"\\xa0\",\"\").replace(\" \",\"\").replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "rows = []  # [year, operatorLabel, gridusagename, price]\n",
    "for path in sorted(DATA_SRC.glob(\"*.csv\")):\n",
    "    df = pd.read_csv(path, dtype=str)\n",
    "    pc  = find_price_col(df.columns)\n",
    "    per = find_period_col(df.columns)\n",
    "    opl = find_operatorlabel_col(df.columns)\n",
    "    gnm = find_gridusagename_col(df.columns)\n",
    "\n",
    "    if pc is None or per is None:\n",
    "        print(f\"[WARN] {path.name}: missing key columns (price/period) → ignored\")\n",
    "        continue\n",
    "\n",
    "    n = len(df)\n",
    "    for i in range(n):\n",
    "        # année\n",
    "        y_raw = str(df.iloc[i][per]) if per in df.columns else \"\"\n",
    "        y_digits = \"\".join(ch for ch in y_raw if ch.isdigit())\n",
    "        year = int(y_digits) if len(y_digits) == 4 else None\n",
    "        if year is None or not (YEAR_MIN <= year <= YEAR_MAX):\n",
    "            continue\n",
    "\n",
    "        op  = df.iloc[i][opl] if opl in df.columns else None\n",
    "        grid= df.iloc[i][gnm] if gnm in df.columns else None\n",
    "        pr  = to_float(df.iloc[i][pc])\n",
    "\n",
    "        # only keeps plausible prices (0–100 pence/kWh)\n",
    "        if pr is None or not (0 < pr < 100):\n",
    "            continue\n",
    "\n",
    "        rows.append([year, op, grid, pr])\n",
    "\n",
    "print(f\"Valid lines collected : {len(rows)}\")\n",
    "\n",
    "# Sorting & outputs\n",
    "rows.sort(key=lambda r: (r[0], str(r[1]) if r[1] else \"\", str(r[2]) if r[2] else \"\"))\n",
    "\n",
    "raw_out = DATA_INTER / \"elcom_prices_raw_minimal_2014_2025.csv\"\n",
    "agg_out = DATA_INTER / \"elcom_prices_by_operator_year_2014_2025.csv\"\n",
    "\n",
    "# (a) all the lines\n",
    "with open(raw_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"year\",\"operatorLabel\",\"gridusagename\",\"price_after_discount_cts_per_kwh\"])\n",
    "    w.writerows(rows)\n",
    "\n",
    "# (b) Average per (year, operatorLabel)\n",
    "agg = {}\n",
    "for year, op, grid, price in rows:\n",
    "    key = (year, op)\n",
    "    if key not in agg:\n",
    "        agg[key] = [0.0, 0]\n",
    "    agg[key][0] += price\n",
    "    agg[key][1] += 1\n",
    "\n",
    "with open(agg_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"year\",\"operatorLabel\",\"avg_price_after_discount_cts_per_kwh\",\"n_tariffs\"])\n",
    "    for (year, op), (s, c) in sorted(agg.items(), key=lambda x: (x[0][0], str(x[0][1]))):\n",
    "        w.writerow([year, op, round(s/c, 6) if c else None, c])\n",
    "\n",
    "print(\"Files :\", raw_out.name, \"|\", agg_out.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "offensive-liberal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: motorization_rate_clean_full.csv | motorization_rate_2015_2024.csv\n",
      "Number of canton codes found in the data (not cleaned yet): 946 / 26\n",
      "Years (full): 1970 → 2024\n",
      "Years (subset): 2015 → 2024\n"
     ]
    }
   ],
   "source": [
    "# Motorisation rate (per 1000 inhabitants)\n",
    "import re, csv\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2015, 2024  # same analysis window as the rest\n",
    "\n",
    "xlsx_path = RAW / \"motorisation_rate.xlsx\"\n",
    "assert xlsx_path.exists(), f\"File not found: {xlsx_path}\"\n",
    "df = pd.read_excel(xlsx_path, dtype=str)\n",
    "\n",
    "assert not df.empty, \"motorisation_rate.xlsx : blank or incorrect sheet.\"\n",
    "\n",
    "# 1) Detection of columns\n",
    "first_col = df.columns[0]\n",
    "year_cols = []\n",
    "for c in df.columns[1:]:\n",
    "    s = re.sub(r\"\\D\", \"\", str(c))  \n",
    "    if len(s) == 4:\n",
    "        year_cols.append(c)\n",
    "\n",
    "assert year_cols, \"No year column detected (with 4 numbers).\"\n",
    "\n",
    "# 2) Subset & melt (wide -> long)\n",
    "df = df[[first_col] + year_cols].copy()\n",
    "df = df.rename(columns={first_col: \"unit\"})\n",
    "\n",
    "long_df = df.melt(id_vars=[\"unit\"], value_vars=year_cols,\n",
    "                  var_name=\"year\", value_name=\"motorization_rate_raw\")\n",
    "\n",
    "# 3) Simple cleaning\n",
    "def clean_text(x):\n",
    "    return re.sub(r\"\\s+\", \" \", str(x).strip())\n",
    "\n",
    "long_df[\"unit\"] = long_df[\"unit\"].apply(clean_text)\n",
    "long_df[\"year\"] = long_df[\"year\"].apply(lambda x: int(re.sub(r\"\\D\", \"\", str(x))) if re.sub(r\"\\D\", \"\", str(x)) else None)\n",
    "\n",
    "# Values: remove non-breaking spaces, replace decimal point, drop markers (*, …, .)\n",
    "def to_float(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).replace(\"\\xa0\",\"\").replace(\" \", \"\")\n",
    "    s = s.replace(\"…\",\"\").replace(\"...\",\"\").replace(\"*\",\"\").replace(\",\", \".\")\n",
    "    s = re.sub(r\"[^0-9.\\-]\", \"\", s)\n",
    "    try:\n",
    "        return float(s) if s != \"\" else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "long_df[\"motorization_rate_per_1000\"] = long_df[\"motorization_rate_raw\"].apply(to_float)\n",
    "\n",
    "# 4) Remove aggregates (Total, Regions, Switzerland, etc.)\n",
    "AGG_RX = re.compile(r\"^(total|région|region|espace|suisse)\\b\", flags=re.IGNORECASE)\n",
    "clean_df = long_df[~long_df[\"unit\"].str.match(AGG_RX, na=False)].copy()\n",
    "\n",
    "# 5) Mapping of cantons through ISO-2 code (with common variants)\n",
    "CANTON_MAP = {\n",
    "    \"Zurich\":\"ZH\",\"Zürich\":\"ZH\",\n",
    "    \"Berne\":\"BE\",\"Bern\":\"BE\",\n",
    "    \"Lucerne\":\"LU\",\"Luzern\":\"LU\",\n",
    "    \"Uri\":\"UR\",\n",
    "    \"Schwyz\":\"SZ\",\n",
    "    \"Obwald\":\"OW\",\"Obwalden\":\"OW\",\n",
    "    \"Nidwald\":\"NW\",\"Nidwalden\":\"NW\",\n",
    "    \"Glaris\":\"GL\",\"Glarus\":\"GL\",\n",
    "    \"Zoug\":\"ZG\",\"Zug\":\"ZG\",\n",
    "    \"Fribourg\":\"FR\",\"Freiburg\":\"FR\",\n",
    "    \"Soleure\":\"SO\",\"Solothurn\":\"SO\",\n",
    "    \"Bâle-Ville\":\"BS\",\"Basel-Stadt\":\"BS\",\"Basel Stadt\":\"BS\",\n",
    "    \"Bâle-Campagne\":\"BL\",\"Basel-Landschaft\":\"BL\",\"Basel Landschaft\":\"BL\",\n",
    "    \"Schaffhouse\":\"SH\",\"Schaffhausen\":\"SH\",\n",
    "    \"Appenzell Rh.-Ext\":\"AR\",\"Appenzell Ausserrhoden\":\"AR\",\"Appenzell Rh.-Ext.\":\"AR\",\n",
    "    \"Appenzell Rh.-Int\":\"AI\",\"Appenzell Innerrhoden\":\"AI\",\"Appenzell Rh.-Int.\":\"AI\",\n",
    "    \"Saint-Gall\":\"SG\",\"St. Gallen\":\"SG\",\"Sankt Gallen\":\"SG\",\n",
    "    \"Grisons\":\"GR\",\"Graubünden\":\"GR\",\"Grigioni\":\"GR\",\n",
    "    \"Argovie\":\"AG\",\"Aargau\":\"AG\",\n",
    "    \"Thurgovie\":\"TG\",\"Thurgau\":\"TG\",\n",
    "    \"Tessin\":\"TI\",\"Ticino\":\"TI\",\n",
    "    \"Vaud\":\"VD\",\n",
    "    \"Valais\":\"VS\",\"Wallis\":\"VS\",\n",
    "    \"Neuchâtel\":\"NE\",\"Neuchatel\":\"NE\",\n",
    "    \"Genève\":\"GE\",\"Geneve\":\"GE\",\"Genf\":\"GE\",\n",
    "    \"Jura\":\"JU\",\n",
    "}\n",
    "\n",
    "clean_df[\"canton\"] = clean_df[\"unit\"].map(lambda s: CANTON_MAP.get(s, s))\n",
    "# canton_code if mapped, otherwise None (added later)\n",
    "REV = {k:v for k,v in CANTON_MAP.items()}\n",
    "def to_code(name):\n",
    "    code = REV.get(name, None)  # if the name has a FR code\n",
    "    if code: return code\n",
    "    # if 'canton' already has a code (2 letters), keep it\n",
    "    if isinstance(name, str) and re.fullmatch(r\"[A-Z]{2}\", name):\n",
    "        return name\n",
    "    # if name is a German form already mapped above, nothing to do here\n",
    "    return CANTON_MAP.get(name, None)\n",
    "\n",
    "# We want two columns: district (FR label as in the file), district_code (ISO-2)\n",
    "clean_df[\"canton_code\"] = clean_df[\"unit\"].apply(lambda s: CANTON_MAP.get(s, None))\n",
    "\n",
    "# Keep only lines with a year and a numerical value\n",
    "clean_df = clean_df[clean_df[\"year\"].notna()].copy()\n",
    "clean_df = clean_df.dropna(subset=[\"motorization_rate_per_1000\"])\n",
    "\n",
    "# Exports\n",
    "cols_out = [\"canton_code\",\"unit\",\"year\",\"motorization_rate_per_1000\"]\n",
    "full_out   = INTER / \"motorization_rate_clean_full.csv\"\n",
    "subset_out = INTER / f\"motorization_rate_{YEAR_MIN}_{YEAR_MAX}.csv\"\n",
    "\n",
    "# Full\n",
    "with open(full_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"canton_code\",\"canton\",\"year\",\"motorization_rate_per_1000\"])\n",
    "    for r in clean_df.itertuples(index=False):\n",
    "        w.writerow([r.canton_code, r.unit, int(r.year), r.motorization_rate_per_1000])\n",
    "\n",
    "# Window 2015–2024\n",
    "subset = clean_df[(clean_df[\"year\"]>=YEAR_MIN) & (clean_df[\"year\"]<=YEAR_MAX)].copy()\n",
    "with open(subset_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"canton_code\",\"canton\",\"year\",\"motorization_rate_per_1000\"])\n",
    "    for r in subset.itertuples(index=False):\n",
    "        w.writerow([r.canton_code, r.unit, int(r.year), r.motorization_rate_per_1000])\n",
    "\n",
    "# Diagnostics\n",
    "print(\"Saved as:\", full_out.name, \"|\", subset_out.name)\n",
    "print(\"Number of canton codes found in the data (not cleaned yet):\", clean_df[\"canton_code\"].notna().sum(), \"/\", clean_df[\"unit\"].nunique())\n",
    "print(\"Years (full):\", int(clean_df[\"year\"].min()), \"→\", int(clean_df[\"year\"].max()))\n",
    "if not subset.empty:\n",
    "    print(\"Years (subset):\", int(subset[\"year\"].min()), \"→\", int(subset[\"year\"].max()))\n",
    "else:\n",
    "    print(\"No points in the window\", YEAR_MIN, \"→\", YEAR_MAX, \"(it's fine if the file stops before that).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "collected-folks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: gdp_per_capita_clean_full.csv | gdp_per_capita_2015_2024.csv\n",
      "Years going from 2008 → 2025\n",
      "Cantons: 26\n"
     ]
    }
   ],
   "source": [
    "# GDP per capita by Canton (2008–2022) -> we extrapolate for 2023–2025 because of the lack of available data\n",
    "import pandas as pd, re, csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2015, 2024  \n",
    "xlsx_path = RAW / \"gdp_per_capita.xlsx\"\n",
    "assert xlsx_path.exists(), f\"Fichier introuvable: {xlsx_path}\"\n",
    "\n",
    "df = pd.read_excel(xlsx_path, dtype=str)\n",
    "\n",
    "# Column detection\n",
    "first_col = df.columns[0]\n",
    "year_cols = [c for c in df.columns[1:] if re.sub(r\"\\D\", \"\", str(c)).isdigit()]\n",
    "\n",
    "# Wide -> Long\n",
    "df = df[[first_col] + year_cols].rename(columns={first_col: \"unit\"})\n",
    "long_df = df.melt(id_vars=[\"unit\"], value_vars=year_cols,\n",
    "                  var_name=\"year\", value_name=\"gdp_pc_raw\")\n",
    "\n",
    "# Cleaning\n",
    "def to_float(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).replace(\"\\xa0\", \"\").replace(\"\\u202f\",\"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    s = re.sub(r\"[^0-9.\\-]\", \"\", s)\n",
    "    try:\n",
    "        return float(s) if s else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "long_df[\"year\"] = long_df[\"year\"].apply(\n",
    "    lambda x: int(re.sub(r\"\\D\", \"\", str(x))) if re.sub(r\"\\D\", \"\", str(x)) else None\n",
    ")\n",
    "long_df[\"gdp_per_capita_chf\"] = long_df[\"gdp_pc_raw\"].apply(to_float)\n",
    "long_df = long_df.dropna(subset=[\"year\", \"gdp_per_capita_chf\"])\n",
    "\n",
    "# Remove the national aggregate \"Switzerland\"\n",
    "long_df = long_df[~long_df[\"unit\"].str.match(r\"^suisse\\b\", case=False, na=False)].copy()\n",
    "\n",
    "# Mapping cantons FR/DE -> codes ISO-2\n",
    "CANTON_MAP = {\n",
    "    \"Zurich\":\"ZH\",\"Zürich\":\"ZH\",\n",
    "    \"Berne\":\"BE\",\"Bern\":\"BE\",\n",
    "    \"Lucerne\":\"LU\",\"Luzern\":\"LU\",\n",
    "    \"Uri\":\"UR\",\n",
    "    \"Schwyz\":\"SZ\",\n",
    "    \"Obwald\":\"OW\",\"Obwalden\":\"OW\",\n",
    "    \"Nidwald\":\"NW\",\"Nidwalden\":\"NW\",\n",
    "    \"Glaris\":\"GL\",\"Glarus\":\"GL\",\n",
    "    \"Zoug\":\"ZG\",\"Zug\":\"ZG\",\n",
    "    \"Fribourg\":\"FR\",\"Freiburg\":\"FR\",\n",
    "    \"Soleure\":\"SO\",\"Solothurn\":\"SO\",\n",
    "    \"Bâle-Ville\":\"BS\",\"Basel-Stadt\":\"BS\",\"Basel Stadt\":\"BS\",\n",
    "    \"Bâle-Campagne\":\"BL\",\"Basel-Landschaft\":\"BL\",\"Basel Landschaft\":\"BL\",\n",
    "    \"Schaffhouse\":\"SH\",\"Schaffhausen\":\"SH\",\n",
    "    \"Appenzell Rhodes-Extérieures\":\"AR\",\"Appenzell Rhodes Extérieures\":\"AR\",\"Appenzell Rh.-Ext.\":\"AR\",\"Appenzell Ausserrhoden\":\"AR\",\n",
    "    \"Appenzell Rhodes-Intérieures\":\"AI\",\"Appenzell Rhodes Intérieures\":\"AI\",\"Appenzell Rh.-Int.\":\"AI\",\"Appenzell Innerrhoden\":\"AI\",\n",
    "    \"Saint-Gall\":\"SG\",\"St. Gallen\":\"SG\",\"Sankt Gallen\":\"SG\",\n",
    "    \"Grisons\":\"GR\",\"Graubünden\":\"GR\",\"Grigioni\":\"GR\",\n",
    "    \"Argovie\":\"AG\",\"Aargau\":\"AG\",\n",
    "    \"Thurgovie\":\"TG\",\"Thurgau\":\"TG\",\n",
    "    \"Tessin\":\"TI\",\"Ticino\":\"TI\",\n",
    "    \"Vaud\":\"VD\",\n",
    "    \"Valais\":\"VS\",\"Wallis\":\"VS\",\n",
    "    \"Neuchâtel\":\"NE\",\"Neuchatel\":\"NE\",\n",
    "    \"Genève\":\"GE\",\"Geneve\":\"GE\",\"Genf\":\"GE\",\n",
    "    \"Jura\":\"JU\"\n",
    "}\n",
    "long_df[\"canton\"] = long_df[\"unit\"].astype(str).str.strip()\n",
    "long_df[\"canton_code\"] = long_df[\"canton\"].map(CANTON_MAP)\n",
    "\n",
    "# Extrapolation for 2023–2025 (CAGR of 3 last years per canton, fallback of 2%)\n",
    "extrapolated_rows = []\n",
    "for c, g in long_df.groupby(\"canton\"):\n",
    "    g = g.sort_values(\"year\")\n",
    "    # Last 3 observed years that we have\n",
    "    tail = g.tail(3)\n",
    "    if len(tail) >= 2:\n",
    "        # CAGR: (v_t / v_{t-k})**(1/k) - 1 ; if values are negative, fallback\n",
    "        v0, vt = tail[\"gdp_per_capita_chf\"].iloc[0], tail[\"gdp_per_capita_chf\"].iloc[-1]\n",
    "        k = (tail[\"year\"].iloc[-1] - tail[\"year\"].iloc[0]) or 1\n",
    "        if v0 and v0 > 0 and vt and vt > 0:\n",
    "            mean_growth = (vt / v0) ** (1 / k) - 1.0\n",
    "        else:\n",
    "            mean_growth = 0.02\n",
    "    else:\n",
    "        mean_growth = 0.02\n",
    "\n",
    "    last_val = g[\"gdp_per_capita_chf\"].iloc[-1]\n",
    "    for y in [2023, 2024, 2025]:\n",
    "        last_val = last_val * (1 + mean_growth)\n",
    "        extrapolated_rows.append([c, g[\"canton_code\"].iloc[0], y, last_val])\n",
    "\n",
    "extra_df = pd.DataFrame(\n",
    "    extrapolated_rows,\n",
    "    columns=[\"canton\", \"canton_code\", \"year\", \"gdp_per_capita_chf\"]\n",
    ")\n",
    "\n",
    "clean_df = pd.concat(\n",
    "    [long_df[[\"canton\",\"canton_code\",\"year\",\"gdp_per_capita_chf\"]], extra_df],\n",
    "    ignore_index=True\n",
    ").sort_values([\"canton\",\"year\"])\n",
    "\n",
    "# Exports\n",
    "full_out   = INTER / \"gdp_per_capita_clean_full.csv\"\n",
    "subset_out = INTER / f\"gdp_per_capita_{YEAR_MIN}_{YEAR_MAX}.csv\"\n",
    "\n",
    "with open(full_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"canton_code\",\"canton\",\"year\",\"gdp_per_capita_chf\"])\n",
    "    for r in clean_df.itertuples(index=False):\n",
    "        w.writerow([r.canton_code, r.canton, int(r.year), r.gdp_per_capita_chf])\n",
    "\n",
    "subset = clean_df[(clean_df[\"year\"]>=YEAR_MIN) & (clean_df[\"year\"]<=YEAR_MAX)]\n",
    "with open(subset_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"canton_code\",\"canton\",\"year\",\"gdp_per_capita_chf\"])\n",
    "    for r in subset.itertuples(index=False):\n",
    "        w.writerow([r.canton_code, r.canton, int(r.year), r.gdp_per_capita_chf])\n",
    "\n",
    "print(\"Saved as:\", full_out.name, \"|\", subset_out.name)\n",
    "print(\"Years going from\", int(clean_df[\"year\"].min()), \"→\", int(clean_df[\"year\"].max()))\n",
    "print(\"Cantons:\", clean_df[\"canton\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "described-keyboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\hamza\\OneDrive\\Desktop\\projet-ada-hk\\data\\raw\\Bilan_pop_CH.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: population_balance_cantons_clean_full.csv | population_balance_cantons_2015_2024.csv\n",
      "Parsed sheets: [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Years going from 1999 → 2024 | Cantons: 26 | Lignes: 676\n",
      "Cover 2015–2024: 2015 → 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\741677251.py:79: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n"
     ]
    }
   ],
   "source": [
    "# Population balance by canton \n",
    "import pandas as pd, re, csv\n",
    "from pathlib import Path\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2015, 2024\n",
    "xlsx_path = RAW / \"Bilan_pop_CH.xlsx\"\n",
    "print(\"Reading:\", xlsx_path.resolve())\n",
    "assert xlsx_path.exists(), f\"Can't find: {xlsx_path}\"\n",
    "\n",
    "def norm_txt(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).replace(\"\\xa0\",\" \").strip()\n",
    "    return re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "def to_float(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).replace(\"\\u202f\",\"\").replace(\"\\xa0\",\"\").replace(\" \", \"\")\n",
    "    s = s.replace(\"…\",\"\").replace(\"...\",\"\").replace(\"*\",\"\").replace(\",\", \".\")\n",
    "    s = re.sub(r\"[^0-9.\\-]\", \"\", s)\n",
    "    try: return float(s) if s else None\n",
    "    except: return None\n",
    "\n",
    "# Canton map (FR/DE/IT -> ISO-2)\n",
    "CANTON_MAP = {\n",
    "    \"Zurich\":\"ZH\",\"Zürich\":\"ZH\",\n",
    "    \"Berne\":\"BE\",\"Bern\":\"BE\",\n",
    "    \"Lucerne\":\"LU\",\"Luzern\":\"LU\",\n",
    "    \"Uri\":\"UR\",\n",
    "    \"Schwyz\":\"SZ\",\n",
    "    \"Obwald\":\"OW\",\"Obwalden\":\"OW\",\"Obwald.\":\"OW\",\n",
    "    \"Nidwald\":\"NW\",\"Nidwalden\":\"NW\",\"Nidwald.\":\"NW\",\n",
    "    \"Glaris\":\"GL\",\"Glarus\":\"GL\",\n",
    "    \"Zoug\":\"ZG\",\"Zug\":\"ZG\",\n",
    "    \"Fribourg\":\"FR\",\"Freiburg\":\"FR\",\n",
    "    \"Soleure\":\"SO\",\"Solothurn\":\"SO\",\n",
    "    \"Bâle-Ville\":\"BS\",\"Basel-Stadt\":\"BS\",\"Basel Stadt\":\"BS\",\n",
    "    \"Bâle-Campagne\":\"BL\",\"Basel-Landschaft\":\"BL\",\"Basel Landschaft\":\"BL\",\n",
    "    \"Schaffhouse\":\"SH\",\"Schaffhausen\":\"SH\",\n",
    "    \"Appenzell Rhodes-Extérieures\":\"AR\",\"Appenzell Rh.-Ext.\":\"AR\",\"Appenzell Ausserrhoden\":\"AR\",\n",
    "    \"Appenzell Rhodes-Intérieures\":\"AI\",\"Appenzell Rh.-Int.\":\"AI\",\"Appenzell Innerrhoden\":\"AI\",\n",
    "    \"Saint-Gall\":\"SG\",\"St. Gallen\":\"SG\",\"Sankt Gallen\":\"SG\",\n",
    "    \"Grisons\":\"GR\",\"Graubünden\":\"GR\",\"Grigioni\":\"GR\",\n",
    "    \"Argovie\":\"AG\",\"Aargau\":\"AG\",\n",
    "    \"Thurgovie\":\"TG\",\"Thurgau\":\"TG\",\n",
    "    \"Tessin\":\"TI\",\"Ticino\":\"TI\",\n",
    "    \"Vaud\":\"VD\",\n",
    "    \"Valais\":\"VS\",\"Wallis\":\"VS\",\n",
    "    \"Neuchâtel\":\"NE\",\"Neuchatel\":\"NE\",\n",
    "    \"Genève\":\"GE\",\"Geneve\":\"GE\",\"Genf\":\"GE\",\n",
    "    \"Jura\":\"JU\",\n",
    "}\n",
    "CANTON_SET = set(CANTON_MAP.keys())\n",
    "\n",
    "# Target columns -> regex (merger/variant tolerant)\n",
    "COL_PATTERNS = {\n",
    "    \"pop_jan1\":        re.compile(r\"(état|etat).*(1er|premier).*(janvier|jan)\\b\", re.I),\n",
    "    \"births\":          re.compile(r\"naissance\", re.I),\n",
    "    \"deaths\":          re.compile(r\"d[ée]c[èe]s|deces\", re.I),\n",
    "    \"natural_increase\":re.compile(r\"accroissement.*naturel\", re.I),\n",
    "    \"arrivals\":        re.compile(r\"arriv\", re.I),\n",
    "    \"departures\":      re.compile(r\"d[ée]part\", re.I),\n",
    "    \"net_migration\":   re.compile(r\"solde.*migr\", re.I),\n",
    "    \"pop_dec31\":       re.compile(r\"(état|etat).*(d[ée]cembre|dec)\\b\", re.I),\n",
    "    \"variation_abs\":   re.compile(r\"variation.*nombres.*absolus|variation\\s*en\\s*nombres\", re.I),\n",
    "    \"variation_pct\":   re.compile(r\"(variation.*%|en\\s*%)\", re.I),\n",
    "}\n",
    "\n",
    "xls = pd.ExcelFile(xlsx_path)\n",
    "sheet_years = sorted(\n",
    "    [(sh, int(m.group(1))) for sh in xls.sheet_names\n",
    "     if (m := re.match(r\"^Cantons\\s*\\((\\d{4})\\)\", sh))],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for sheet_name, year in sheet_years:\n",
    "    df0 = pd.read_excel(xlsx_path, sheet_name=sheet_name, header=None, dtype=str)\n",
    "    df0 = df0.applymap(norm_txt)\n",
    "\n",
    "    # Builds a concatenated header with the first 5 lines (merge safe)\n",
    "    hdr_n = min(5, len(df0))\n",
    "    head = df0.iloc[:hdr_n, :].fillna(\"\")\n",
    "    col_labels = []\n",
    "    for c in range(df0.shape[1]):\n",
    "        tokens = [t for t in head.iloc[:, c].tolist() if t]\n",
    "        col_labels.append(\" | \".join(tokens))\n",
    "\n",
    "    # Canton column = the one containing \"Canton\" in the header, otherwise the first one\n",
    "    unit_col = 0\n",
    "    for j, lab in enumerate(col_labels):\n",
    "        if re.search(r\"\\bcanton[s]?\\b\", lab, re.I):\n",
    "            unit_col = j; break\n",
    "\n",
    "    # maps numeric columns via patterns\n",
    "    col_map = {\"unit\": unit_col}\n",
    "    for key, rx in COL_PATTERNS.items():\n",
    "        for j, lab in enumerate(col_labels):\n",
    "            if rx.search(lab):\n",
    "                col_map[key] = j\n",
    "                break\n",
    "\n",
    "    data = df0.iloc[hdr_n:, :].reset_index(drop=True)\n",
    "\n",
    "    # keeps only the 26 cantons (exact match on source wording)\n",
    "    for i in range(len(data)):\n",
    "        unit = data.iat[i, unit_col] if unit_col < data.shape[1] else \"\"\n",
    "        if not unit or unit not in CANTON_SET:\n",
    "            continue\n",
    "        canton = unit\n",
    "        code = CANTON_MAP[canton]\n",
    "\n",
    "        def getv(k):\n",
    "            j = col_map.get(k, None)\n",
    "            if j is None or j >= data.shape[1]: return None\n",
    "            return to_float(data.iat[i, j])\n",
    "\n",
    "        rows.append([\n",
    "            code, canton, year,\n",
    "            getv(\"pop_jan1\"),\n",
    "            getv(\"births\"),\n",
    "            getv(\"deaths\"),\n",
    "            getv(\"natural_increase\"),\n",
    "            getv(\"arrivals\"),\n",
    "            getv(\"departures\"),\n",
    "            getv(\"net_migration\"),\n",
    "            getv(\"pop_dec31\"),\n",
    "            getv(\"variation_abs\"),\n",
    "            getv(\"variation_pct\"),\n",
    "        ])\n",
    "\n",
    "cols = [\"canton_code\",\"canton\",\"year\",\"pop_jan1\",\"births\",\"deaths\",\"natural_increase\",\n",
    "        \"arrivals\",\"departures\",\"net_migration\",\"pop_dec31\",\"variation_abs\",\"variation_pct\"]\n",
    "out = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "# Security here: valid values and years only\n",
    "measures = cols[3:]\n",
    "out = out.dropna(subset=measures, how=\"all\")\n",
    "out = out.dropna(subset=[\"year\"]).copy()\n",
    "out[\"year\"] = out[\"year\"].astype(int)\n",
    "out = out.sort_values([\"canton\",\"year\"]).reset_index(drop=True)\n",
    "\n",
    "full_out   = INTER / \"population_balance_cantons_clean_full.csv\"\n",
    "subset_out = INTER / f\"population_balance_cantons_{YEAR_MIN}_{YEAR_MAX}.csv\"\n",
    "\n",
    "with open(full_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow(cols)\n",
    "    for r in out.itertuples(index=False):\n",
    "        w.writerow(list(r))\n",
    "\n",
    "subset = out[(out[\"year\"]>=YEAR_MIN) & (out[\"year\"]<=YEAR_MAX)]\n",
    "with open(subset_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow(cols)\n",
    "    for r in subset.itertuples(index=False):\n",
    "        w.writerow(list(r))\n",
    "\n",
    "print(\"Saved as:\", full_out.name, \"|\", subset_out.name)\n",
    "print(\"Parsed sheets:\", [y for _, y in sheet_years])\n",
    "print(\"Years going from\", out[\"year\"].min(), \"→\", out[\"year\"].max(), \"| Cantons:\", out[\"canton\"].nunique(), \"| Lignes:\", len(out))\n",
    "print(\"Cover 2015–2024:\", subset[\"year\"].min() if not subset.empty else None, \"→\", subset[\"year\"].max() if not subset.empty else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "decimal-audit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: C:\\Users\\hamza\\OneDrive\\Desktop\\projet-ada-hk\\data\\raw\\policy_parties_cantons.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as: cantonal_parliaments_greens_clean_full.csv | cantonal_parliaments_greens_2015_2024.csv\n",
      "Years (full): 2014 → 2025\n",
      "Cantons: 26 | Lines: 221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_7232\\3714569568.py:82: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df0 = df0.applymap(norm_txt)\n"
     ]
    }
   ],
   "source": [
    "# Cantonal parliaments: seats by party & sex (extract Greens) \n",
    "import pandas as pd, re, csv\n",
    "from pathlib import Path\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2014, 2025\n",
    "xlsx_path = RAW / \"policy_parties_cantons.xlsx\"\n",
    "print(\"Reading:\", xlsx_path.resolve())\n",
    "assert xlsx_path.exists(), f\"Can't find: {xlsx_path}\"\n",
    "\n",
    "def norm_txt(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).replace(\"\\xa0\",\" \").strip()\n",
    "    return re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "def to_int_or_none(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x)\n",
    "    s = s.replace(\"\\u202f\",\"\").replace(\"\\xa0\",\"\").replace(\" \", \"\")\n",
    "    s = s.replace(\"…\",\"\").replace(\"...\",\"\").replace(\"*\",\"\").replace(\",\", \".\")\n",
    "    s = re.sub(r\"[^0-9.\\-]\", \"\", s)\n",
    "    if s == \"\": return None\n",
    "    try:\n",
    "        # some tables indicate absence with \".\", I have removed it\n",
    "        return int(float(s))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# 26 cantons (FR/DE/IT -> ISO-2)\n",
    "CANTON_MAP = {\n",
    "    \"Zurich\":\"ZH\",\"Zürich\":\"ZH\",\n",
    "    \"Berne\":\"BE\",\"Bern\":\"BE\",\n",
    "    \"Lucerne\":\"LU\",\"Luzern\":\"LU\",\n",
    "    \"Uri\":\"UR\",\n",
    "    \"Schwyz\":\"SZ\",\n",
    "    \"Obwald\":\"OW\",\"Obwalden\":\"OW\",\n",
    "    \"Nidwald\":\"NW\",\"Nidwalden\":\"NW\",\n",
    "    \"Glaris\":\"GL\",\"Glarus\":\"GL\",\n",
    "    \"Zoug\":\"ZG\",\"Zug\":\"ZG\",\n",
    "    \"Fribourg\":\"FR\",\"Freiburg\":\"FR\",\n",
    "    \"Soleure\":\"SO\",\"Solothurn\":\"SO\",\n",
    "    \"Bâle-Ville\":\"BS\",\"Basel-Stadt\":\"BS\",\"Basel Stadt\":\"BS\",\n",
    "    \"Bâle-Campagne\":\"BL\",\"Basel-Landschaft\":\"BL\",\"Basel Landschaft\":\"BL\",\n",
    "    \"Schaffhouse\":\"SH\",\"Schaffhausen\":\"SH\",\n",
    "    \"Appenzell Rh.-Ext.\":\"AR\",\"Appenzell Ausserrhoden\":\"AR\",\"Appenzell Rhodes-Extérieures\":\"AR\",\n",
    "    \"Appenzell Rh.-Int.\":\"AI\",\"Appenzell Innerrhoden\":\"AI\",\"Appenzell Rhodes-Intérieures\":\"AI\",\n",
    "    \"Saint-Gall\":\"SG\",\"St. Gallen\":\"SG\",\"Sankt Gallen\":\"SG\",\n",
    "    \"Grisons\":\"GR\",\"Graubünden\":\"GR\",\"Grigioni\":\"GR\",\n",
    "    \"Argovie\":\"AG\",\"Aargau\":\"AG\",\n",
    "    \"Thurgovie\":\"TG\",\"Thurgau\":\"TG\",\n",
    "    \"Tessin\":\"TI\",\"Ticino\":\"TI\",\n",
    "    \"Vaud\":\"VD\",\n",
    "    \"Valais\":\"VS\",\"Wallis\":\"VS\",\n",
    "    \"Neuchâtel\":\"NE\",\"Neuchatel\":\"NE\",\n",
    "    \"Genève\":\"GE\",\"Geneve\":\"GE\",\"Genf\":\"GE\",\n",
    "    \"Jura\":\"JU\",\n",
    "}\n",
    "CANTON_SET = set(CANTON_MAP.keys())\n",
    "\n",
    "# regex to identify columns\n",
    "RX_CANTON = re.compile(r\"\\bcanton[s]?\\b\", re.I)\n",
    "RX_YEAR   = re.compile(r\"ann[ée]e.*[ée]lectorale|ann[ée]e\\s*électorale|année électorale\", re.I)\n",
    "# variations of “Vert-e-s” or \"Greens\"\n",
    "RX_GREENS = re.compile(r\"\\b(vert[\\-\\s]*e[\\-\\s]*s|les\\s*verts|verts|pes)\\b\", re.I)\n",
    "RX_SUB_F  = re.compile(r\"\\bF\\b\", re.I)\n",
    "RX_SUB_H  = re.compile(r\"\\bH\\b\", re.I)\n",
    "\n",
    "xls = pd.ExcelFile(xlsx_path)\n",
    "\n",
    "def build_col_labels(df, n_header=6):\n",
    "    n = min(n_header, len(df))\n",
    "    head = df.iloc[:n, :].fillna(\"\")\n",
    "    labels = []\n",
    "    for c in range(df.shape[1]):\n",
    "        toks = [norm_txt(t) for t in head.iloc[:, c].tolist() if norm_txt(t)]\n",
    "        labels.append(\" | \".join(toks))\n",
    "    return labels\n",
    "\n",
    "rows = []\n",
    "\n",
    "for sheet in xls.sheet_names:\n",
    "    df0 = pd.read_excel(xlsx_path, sheet_name=sheet, header=None, dtype=str)\n",
    "    df0 = df0.applymap(norm_txt)\n",
    "    if df0.empty: \n",
    "        continue\n",
    "\n",
    "    labels = build_col_labels(df0, n_header=6)\n",
    "\n",
    "    # Find column Cantons and Year\n",
    "    canton_col, year_col = None, None\n",
    "    for j, lab in enumerate(labels):\n",
    "        if canton_col is None and RX_CANTON.search(lab):\n",
    "            canton_col = j\n",
    "        if year_col is None and RX_YEAR.search(lab):\n",
    "            year_col = j\n",
    "    # If there are no clear clues, we try heuristics: district = column 0\n",
    "    if canton_col is None: canton_col = 0\n",
    "\n",
    "    # Colonnes Greens F/H\n",
    "    greens_F_idx, greens_H_idx = None, None\n",
    "    all_party_indices = []  # to calculate total seats (sum of all F/M)\n",
    "    for j, lab in enumerate(labels):\n",
    "        # identify any F/H subheading column linked to a party\n",
    "        if RX_SUB_F.search(lab) or RX_SUB_H.search(lab):\n",
    "            all_party_indices.append(j)\n",
    "        # F/M marker under Greens\n",
    "        if RX_GREENS.search(lab) and RX_SUB_F.search(lab):\n",
    "            greens_F_idx = j\n",
    "        if RX_GREENS.search(lab) and RX_SUB_H.search(lab):\n",
    "            greens_H_idx = j\n",
    "\n",
    "    # data = lines under the header (the raw data has some text and images in the first cells of the excel, that we need to skip)\n",
    "    start = 6  # skip the 6 merged header lines\n",
    "    data = df0.iloc[start:, :].reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        canton_label = data.iat[i, canton_col] if canton_col < data.shape[1] else \"\"\n",
    "        if not canton_label or canton_label not in CANTON_SET:\n",
    "            continue\n",
    "        code = CANTON_MAP[canton_label]\n",
    "\n",
    "        # year of mandate: if there is a dedicated column, otherwise attempt to extract the first year present on the line\n",
    "        year_val = None\n",
    "        if year_col is not None and year_col < data.shape[1]:\n",
    "            yraw = data.iat[i, year_col]\n",
    "            ydigits = re.findall(r\"\\d{4}\", yraw) if isinstance(yraw, str) else []\n",
    "            year_val = int(ydigits[0]) if ydigits else None\n",
    "\n",
    "        # \"greens\" seats\n",
    "        gF = to_int_or_none(data.iat[i, greens_F_idx]) if greens_F_idx is not None and greens_F_idx < data.shape[1] else None\n",
    "        gH = to_int_or_none(data.iat[i, greens_H_idx]) if greens_H_idx is not None and greens_H_idx < data.shape[1] else None\n",
    "        greens_seats = (gF or 0) + (gH or 0)\n",
    "\n",
    "        # total seats (sum of all F/H columns of parties)\n",
    "        total_seats = 0\n",
    "        for j in all_party_indices:\n",
    "            if j < data.shape[1]:\n",
    "                v = to_int_or_none(data.iat[i, j])\n",
    "                if v is not None:\n",
    "                    total_seats += v\n",
    "\n",
    "        if year_val is None:\n",
    "            continue  # we only keep it if we have the year\n",
    "\n",
    "        rows.append([code, canton_label, year_val, greens_seats, total_seats])\n",
    "\n",
    "# Exports\n",
    "out = pd.DataFrame(rows, columns=[\"canton_code\",\"canton\",\"year\",\"greens_seats\",\"total_seats\"])\n",
    "out = out.dropna(subset=[\"year\"]).copy()\n",
    "out[\"year\"] = out[\"year\"].astype(int)\n",
    "out = out[(out[\"year\"]>=YEAR_MIN) & (out[\"year\"]<=YEAR_MAX)]\n",
    "out[\"greens_share\"] = out.apply(lambda r: (r.greens_seats / r.total_seats) if r.total_seats else None, axis=1)\n",
    "out = out.sort_values([\"canton\",\"year\"]).reset_index(drop=True)\n",
    "\n",
    "full_out   = INTER / \"cantonal_parliaments_greens_clean_full.csv\"\n",
    "subset_out = INTER / \"cantonal_parliaments_greens_2015_2024.csv\"  # pour ton panel\n",
    "\n",
    "# full (2014–2025)\n",
    "with open(full_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow(out.columns)\n",
    "    for r in out.itertuples(index=False):\n",
    "        w.writerow(list(r))\n",
    "\n",
    "# subset (2015–2024)\n",
    "subset = out[(out[\"year\"]>=2015) & (out[\"year\"]<=2024)]\n",
    "with open(subset_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow(out.columns)\n",
    "    for r in subset.itertuples(index=False):\n",
    "        w.writerow(list(r))\n",
    "\n",
    "print(\"Saved as:\", full_out.name, \"|\", subset_out.name)\n",
    "print(\"Years (full):\", out[\"year\"].min() if not out.empty else None, \"→\", out[\"year\"].max() if not out.empty else None)\n",
    "print(\"Cantons:\", out[\"canton\"].nunique(), \"| Lines:\", len(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "enabling-czech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture: C:\\Users\\hamza\\OneDrive\\Desktop\\projet-ada-hk\\data\\raw\\canton_climate_co2.xlsx\n",
      "Saved as canton_climate_co2_clean.csv | canton_climate_co2_panel_2015_2024.csv\n",
      "Cantons: 26 | Years covered: 2015 → 2024\n"
     ]
    }
   ],
   "source": [
    "# Canton-level climate & CO2 (static) -> tidy + panel (2015–2024)\n",
    "import pandas as pd, re, csv\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 2015, 2024\n",
    "xlsx_path = RAW / \"canton_climate_co2.xlsx\"\n",
    "print(\"Lecture:\", xlsx_path.resolve())\n",
    "assert xlsx_path.exists(), f\"Introuvable: {xlsx_path}\"\n",
    "\n",
    "def norm_txt(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().replace(\"\\xa0\",\" \")\n",
    "    return re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "def to_float(x):\n",
    "    if pd.isna(x): return None\n",
    "    s = str(x).replace(\"\\u202f\",\"\").replace(\"\\xa0\",\"\").replace(\" \", \"\").replace(\",\", \".\")\n",
    "    s = re.sub(r\"[^0-9.\\-]\", \"\", s)\n",
    "    try: return float(s) if s else None\n",
    "    except: return None\n",
    "\n",
    "# Mapping canton FR/DE/IT -> code ISO-2\n",
    "CANTON_MAP = {\n",
    "    \"Zurich\":\"ZH\",\"Zürich\":\"ZH\",\n",
    "    \"Berne\":\"BE\",\"Bern\":\"BE\",\n",
    "    \"Lucerne\":\"LU\",\"Luzern\":\"LU\",\n",
    "    \"Uri\":\"UR\",\n",
    "    \"Schwyz\":\"SZ\",\n",
    "    \"Obwald\":\"OW\",\"Obwalden\":\"OW\",\"Obwald.\":\"OW\",\n",
    "    \"Nidwald\":\"NW\",\"Nidwalden\":\"NW\",\"Nidwald.\":\"NW\",\n",
    "    \"Glaris\":\"GL\",\"Glarus\":\"GL\",\n",
    "    \"Zug\":\"ZG\",\"Zoug\":\"ZG\",\n",
    "    \"Fribourg\":\"FR\",\"Freiburg\":\"FR\",\n",
    "    \"Soleure\":\"SO\",\"Solothurn\":\"SO\",\n",
    "    \"Bâle-Ville\":\"BS\",\"Basel-Stadt\":\"BS\",\"Basel Stadt\":\"BS\",\"Basel-Stadt\":\"BS\",\n",
    "    \"Bâle-Campagne\":\"BL\",\"Basel-Landschaft\":\"BL\",\"Basel Landschaft\":\"BL\",\"Basel-Landschaft\":\"BL\",\n",
    "    \"Schaffhouse\":\"SH\",\"Schaffhausen\":\"SH\",\n",
    "    \"Appenzell Rhodes-Extérieures\":\"AR\",\"Appenzell Rh.-Ext.\":\"AR\",\"Appenzell Ausserrhoden\":\"AR\",\"Appenzell A. Rh. Ext.\":\"AR\",\n",
    "    \"Appenzell Rhodes-Intérieures\":\"AI\",\"Appenzell Rh.-Int.\":\"AI\",\"Appenzell Innerrhoden\":\"AI\",\"Appenzell I. Rh. Int.\":\"AI\",\n",
    "    \"Saint-Gall\":\"SG\",\"St. Gallen\":\"SG\",\"Sankt Gallen\":\"SG\",\"St. Gall\":\"SG\",\n",
    "    \"Grisons\":\"GR\",\"Graubünden\":\"GR\",\"Graubuenden\":\"GR\",\"Grigioni\":\"GR\",\"Graubünde\":\"GR\",\n",
    "    \"Argovie\":\"AG\",\"Aargau\":\"AG\",\n",
    "    \"Thurgovie\":\"TG\",\"Thurgau\":\"TG\",\n",
    "    \"Tessin\":\"TI\",\"Ticino\":\"TI\",\n",
    "    \"Vaud\":\"VD\",\n",
    "    \"Valais\":\"VS\",\"Wallis\":\"VS\",\n",
    "    \"Neuchâtel\":\"NE\",\"Neuchatel\":\"NE\",\n",
    "    \"Genève\":\"GE\",\"Geneve\":\"GE\",\"Genf\":\"GE\",\"Geneva\":\"GE\",\n",
    "    \"Jura\":\"JU\",\n",
    "}\n",
    "\n",
    "raw = pd.read_excel(xlsx_path, dtype=str)\n",
    "raw.columns = [norm_txt(c) for c in raw.columns]\n",
    "\n",
    "# detect columns by approximate name (in case the headings change slightly)\n",
    "col_canton = next((c for c in raw.columns if re.search(r\"\\bcanton\\b\", c, re.I)), raw.columns[0])\n",
    "col_summer = next((c for c in raw.columns if re.search(r\"summer|été|ete\", c, re.I)), None)\n",
    "col_winter = next((c for c in raw.columns if re.search(r\"winter|hiver\", c, re.I)), None)\n",
    "col_co2    = next((c for c in raw.columns if re.search(r\"co2|émission|emission\", c, re.I)), None)\n",
    "\n",
    "df = raw[[col_canton, col_summer, col_winter, col_co2]].copy()\n",
    "df.rename(columns={\n",
    "    col_canton: \"canton\",\n",
    "    col_summer: \"summer_temp_c\",\n",
    "    col_winter: \"winter_temp_c\",\n",
    "    col_co2:    \"co2_emissions_mt\"\n",
    "}, inplace=True)\n",
    "\n",
    "# cleaning the values\n",
    "df[\"canton\"] = df[\"canton\"].map(norm_txt)\n",
    "df[\"summer_temp_c\"] = df[\"summer_temp_c\"].apply(to_float)\n",
    "df[\"winter_temp_c\"] = df[\"winter_temp_c\"].apply(to_float)\n",
    "df[\"co2_emissions_mt\"] = df[\"co2_emissions_mt\"].apply(to_float)\n",
    "\n",
    "# map codes & filter for the 26 cantons\n",
    "df[\"canton_code\"] = df[\"canton\"].map(CANTON_MAP)\n",
    "df = df[df[\"canton_code\"].notna()].copy()\n",
    "\n",
    "# static export (canton level)\n",
    "static_out = DATA_INTER / \"canton_climate_co2_clean.csv\"\n",
    "with open(static_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"canton_code\",\"canton\",\"summer_temp_c\",\"winter_temp_c\",\"co2_emissions_mt\"])\n",
    "    for r in df.itertuples(index=False):\n",
    "        w.writerow([r.canton_code, r.canton, r.summer_temp_c, r.winter_temp_c, r.co2_emissions_mt])\n",
    "\n",
    "# Panel 2015–2024 (constant values per canton)\n",
    "years = pd.DataFrame({\"year\": list(range(YEAR_MIN, YEAR_MAX+1))})\n",
    "panel = (df.merge(years, how=\"cross\")[[\"canton_code\",\"canton\",\"year\",\"summer_temp_c\",\"winter_temp_c\",\"co2_emissions_mt\"]]\n",
    "           .sort_values([\"canton\",\"year\"]).reset_index(drop=True))\n",
    "\n",
    "panel_out = DATA_INTER / f\"canton_climate_co2_panel_{YEAR_MIN}_{YEAR_MAX}.csv\"\n",
    "with open(panel_out, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f); w.writerow(panel.columns)\n",
    "    for r in panel.itertuples(index=False):\n",
    "        w.writerow(list(r))\n",
    "\n",
    "print(\"Saved as\", static_out.name, \"|\", panel_out.name)\n",
    "print(\"Cantons:\", df['canton'].nunique(), \"| Years covered:\", YEAR_MIN, \"→\", YEAR_MAX)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
