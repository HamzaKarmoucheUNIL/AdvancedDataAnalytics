{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Robust project root detection (Models)\n",
    "# --------------------------------------------------\n",
    "cwd = Path.cwd().resolve()\n",
    "\n",
    "# Case 1: launched from project root\n",
    "if (cwd / \"data\" / \"intermediate\").exists():\n",
    "    ROOT = cwd\n",
    "\n",
    "# Case 2: launched from notebooks/\n",
    "elif (cwd.parent / \"data\" / \"intermediate\").exists():\n",
    "    ROOT = cwd.parent\n",
    "\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Cannot locate project root from cwd={cwd}. \"\n",
    "        \"Expected 'data/intermediate/' in cwd or parent.\"\n",
    "    )\n",
    "\n",
    "DATA = ROOT / \"data\"\n",
    "INTER = DATA / \"intermediate\"\n",
    "OUT = DATA / \"outputs\"\n",
    "\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"ROOT :\", ROOT)\n",
    "print(\"INTER:\", INTER)\n",
    "print(\"OUT  :\", OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c80b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OUTPUT FOLDERS (FIGURES + TABLES)\n",
    "# ============================================================\n",
    "FIG_DIR = OUT / \"figures\"\n",
    "TAB_DIR = OUT / \"tables\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(filename: str, dpi: int = 200):\n",
    "    \"\"\"Save current matplotlib figure to Outputs/figures/\"\"\"\n",
    "    path = FIG_DIR / filename\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "    print(f\"âœ… Figure saved: {path}\")\n",
    "    plt.close()\n",
    "\n",
    "def save_table(df, filename_csv: str, filename_xlsx: str | None = None):\n",
    "    \"\"\"Save a DataFrame to Outputs/tables/ as CSV (+ optional XLSX).\"\"\"\n",
    "    csv_path = TAB_DIR / filename_csv\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… Table saved: {csv_path}\")\n",
    "    if filename_xlsx:\n",
    "        xlsx_path = TAB_DIR / filename_xlsx\n",
    "        df.to_excel(xlsx_path, index=False)   # requires openpyxl\n",
    "        print(f\"âœ… Table saved: {xlsx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-trinidad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Imports for Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# ============================================================\n",
    "# 0. Setup and loading the data\n",
    "# ============================================================\n",
    "csv_path = INTER / \"master_panel_2015_2024.csv\"\n",
    "assert csv_path.exists(), f\"Missing file: {csv_path}\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "if not csv_path.exists():\n",
    "    raise FileNotFoundError(f\"Can't find: {csv_path}\")\n",
    "\n",
    "print(\"Loading the data...\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Initial setup\n",
    "df_model = df.copy()\n",
    "# Cleaning\n",
    "df_model = df_model.dropna(subset=[\"canton_code\", \"ev_reg_share\"]).reset_index(drop=True)\n",
    "\n",
    "# Target variables and groups\n",
    "df_model[\"ev_reg_share\"] = df_model[\"ev_reg_share\"].astype(float)\n",
    "y = df_model[\"ev_reg_share\"].values\n",
    "df_model[\"trend\"] = df_model[\"year\"] - df_model[\"year\"].min()\n",
    "groups = df_model[\"canton_code\"].values\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Standardised assessment function\n",
    "def evaluate_model(model, X, y, cv, groups, model_name):\n",
    "    r2s, rmses, maes = [], [], []\n",
    "    for train_idx, test_idx in cv.split(X, y, groups):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test  = scaler.transform(X_test)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        r2s.append(r2_score(y_test, preds))\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_test, preds)))\n",
    "        maes.append(mean_absolute_error(y_test, preds))\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"R2\": float(np.mean(r2s)),\n",
    "        \"RMSE\": float(np.mean(rmses)),\n",
    "        \"MAE\": float(np.mean(maes)),\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 1. CLUSTERING \n",
    "# ============================================================\n",
    "print(\"Starting clustering (Strategy for saving linear models)...\")\n",
    "canton_profile = df_model.groupby(\"canton_code\")[[\n",
    "    \"gdp_per_capita_chf\", \"pop_dec31\", \"motorization_rate_per_1000\",\n",
    "    \"summer_temp_c\", \"co2_emissions_mt\"\n",
    "]].mean()\n",
    "\n",
    "# Imputation to avoid KMeans crash\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "canton_profile_clean = pd.DataFrame(\n",
    "    imputer.fit_transform(canton_profile),\n",
    "    columns=canton_profile.columns,\n",
    "    index=canton_profile.index\n",
    ")\n",
    "\n",
    "# K-Means\n",
    "scaler_cluster = StandardScaler()\n",
    "X_cluster = scaler_cluster.fit_transform(canton_profile_clean)\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "canton_profile[\"cluster_id\"] = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Mapping of clusters\n",
    "cluster_map = canton_profile[\"cluster_id\"].to_dict()\n",
    "df_model[\"canton_cluster\"] = df_model[\"canton_code\"].map(cluster_map)\n",
    "cluster_dummies = pd.get_dummies(df_model[\"canton_cluster\"], prefix=\"cluster\", drop_first=True)\n",
    "\n",
    "# ============================================================\n",
    "# 2. FEATURE ENGINEERING & MATRIXES\n",
    "# ============================================================\n",
    "df_model = df_model.fillna(df_model.median(numeric_only=True))\n",
    "\n",
    "df_model[\"income_vs_elec\"] = df_model[\"gdp_per_capita_chf\"] / (df_model[\"price_elcom_cts\"] + 1)\n",
    "df_model[\"mot_x_pop\"] = df_model[\"motorization_rate_per_1000\"] * np.log(df_model[\"pop_dec31\"] + 1)\n",
    "\n",
    "base_features = [\n",
    "    \"motorization_rate_per_1000\", \"gdp_per_capita_chf\", \"price_elcom_cts\",\n",
    "    \"summer_temp_c\", \"winter_temp_c\", \"co2_emissions_mt\", \"greens_share\",\n",
    "    \"pop_dec31\", \"income_vs_elec\", \"mot_x_pop\"\n",
    "]\n",
    "\n",
    "# Linear Matrix (Features + CLUSTERS) -> For Ridge/Lasso\n",
    "X_lin_df = pd.concat([df_model[base_features], cluster_dummies], axis=1)\n",
    "X_lin = X_lin_df.values\n",
    "\n",
    "# Non-Linear Matrix (Features + TREND) -> For Trees/GBR\n",
    "X_nonlin_df = pd.concat([df_model[base_features], df_model[[\"trend\"]]], axis=1)\n",
    "nonlin_features = X_nonlin_df.columns.tolist() \n",
    "X_nonlin = X_nonlin_df.values\n",
    "\n",
    "# ============================================================\n",
    "# 3. MODEL TRAINING\n",
    "# ============================================================\n",
    "results = [] \n",
    "print(\"Training the models...\")\n",
    "\n",
    "# A. Linear Models (Ridge/Lasso handle multicollinearity)\n",
    "results.append(evaluate_model(LinearRegression(), X_nonlin, y, gkf, groups, \"OLS (Simple)\"))\n",
    "results.append(evaluate_model(RidgeCV(alphas=[0.1, 1.0, 10.0, 100.0]), X_lin, y, gkf, groups, \"Ridge CV (Clusters)\"))\n",
    "results.append(evaluate_model(LassoCV(alphas=[0.0001, 0.001, 0.01, 0.1], max_iter=10000), X_lin, y, gkf, groups, \"Lasso CV (Clusters)\"))\n",
    "\n",
    "# B. Non-Linear & Advanced Models\n",
    "results.append(evaluate_model(RandomForestRegressor(n_estimators=300, random_state=42), X_nonlin, y, gkf, groups, \"Random Forest\"))\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(64, 32), activation='relu', solver='adam', alpha=0.01, max_iter=1000, early_stopping=True, random_state=42)\n",
    "results.append(evaluate_model(mlp, X_nonlin, y, gkf, groups, \"Neural Net (MLP)\"))\n",
    "\n",
    "svr = SVR(kernel='rbf', C=10.0, epsilon=0.1)\n",
    "results.append(evaluate_model(svr, X_nonlin, y, gkf, groups, \"SVR (RBF Kernel)\"))\n",
    "\n",
    "kernel = ConstantKernel(1.0) * RBF(length_scale=1.0) + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-10, 1e+1))\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=2, random_state=42)\n",
    "results.append(evaluate_model(gpr, X_nonlin, y, gkf, groups, \"Gaussian Process\"))\n",
    "\n",
    "# C. Gradient Boosting Tuned \n",
    "print(\"Tuning Gradient Boosting (advanced optimization)...\")\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(200, 1000),         # More trees\n",
    "    \"learning_rate\": uniform(0.01, 0.2),\n",
    "    \"max_depth\": randint(2, 5),                 # Shallow trees to avoid overfitting\n",
    "    \"subsample\": uniform(0.6, 0.4),             # Stochastic Gradient Boosting\n",
    "    \"min_samples_split\": randint(2, 10),\n",
    "    \"min_samples_leaf\": randint(2, 10),\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],     # Randomization of features\n",
    "    \"n_iter_no_change\": randint(10, 20),        # Early Stopping (stops if no amelioration)\n",
    "    \"validation_fraction\": uniform(0.1, 0.1)    # Data for the early stopping\n",
    "}\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "search = RandomizedSearchCV(gbr, param_dist, n_iter=25, cv=gkf, scoring=\"r2\", n_jobs=-1, verbose=0, random_state=42)\n",
    "search.fit(X_nonlin, y, groups=groups)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "results.append(evaluate_model(best_model, X_nonlin, y, gkf, groups, \"Gradient Boosting (Tuned)\"))\n",
    "\n",
    "# ============================================================\n",
    "# 4. EXPORT\n",
    "# ============================================================\n",
    "df_models = pd.DataFrame(results)\n",
    "order = [\"OLS (Simple)\", \"Ridge CV (Clusters)\", \"Lasso CV (Clusters)\", \"Random Forest\", \n",
    "         \"Neural Net (MLP)\", \"SVR (RBF Kernel)\", \"Gaussian Process\", \"Gradient Boosting (Tuned)\"]\n",
    "df_models[\"order\"] = df_models[\"Model\"].apply(lambda x: order.index(x) if x in order else 99)\n",
    "df_models = df_models.sort_values(\"order\").drop(columns=\"order\")\n",
    "\n",
    "print(\"\\n=== TABLE OF RESULTS ===\")\n",
    "print(df_models)\n",
    "save_table(df_models, \"Model results.csv\", \"Model results.xlsx\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. SHAP & OLS (Diagnostic)\n",
    "# ============================================================\n",
    "print(\"\\nGenerating the SHAP graph...\")\n",
    "best_model.fit(X_nonlin, y)\n",
    "X_analysis = pd.DataFrame(X_nonlin, columns=nonlin_features)\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_analysis)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Factors (SHAP)\")\n",
    "shap.summary_plot(shap_values, X_analysis, show=False)\n",
    "save_fig(\"shap graph.png\", dpi=300)\n",
    "\n",
    "print(\"\\n=== OLS Statistical analysis (Multicollinearity & P-values) ===\")\n",
    "X_sm = sm.add_constant(X_analysis)\n",
    "model_sm = sm.OLS(y, X_sm).fit()\n",
    "print(model_sm.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. TABLEAU FINAL & EXPORT\n",
    "# ============================================================\n",
    "\n",
    "# Transformation de la liste de rÃ©sultats en DataFrame\n",
    "df_models = pd.DataFrame(results)\n",
    "\n",
    "# Ordre logique pour le rapport (du plus simple au plus complexe/nouveau)\n",
    "order = [\n",
    "    \"OLS (Simple)\",              # Baseline\n",
    "    \"Ridge CV (Clusters)\",       # LinÃ©aire avec correction\n",
    "    \"Lasso CV (Clusters)\",       # LinÃ©aire avec sÃ©lection\n",
    "    \"Random Forest\",             # Arbre standard\n",
    "    \"Gradient Boosting (Tuned)\", # Le \"Champion\"\n",
    "    \"Neural Net (MLP)\",          # Deep Learning (ComplexitÃ©)\n",
    "    \"SVR (RBF Kernel)\",          # Demande TA\n",
    "    \"Gaussian Process\"           # Advanced Analytics\n",
    "]\n",
    "\n",
    "# Application du tri personnalisÃ©\n",
    "# (On gÃ¨re le cas oÃ¹ un modÃ¨le serait absent avec une valeur par dÃ©faut 99)\n",
    "df_models[\"order\"] = df_models[\"Model\"].apply(lambda x: order.index(x) if x in order else 99)\n",
    "df_models = df_models.sort_values(\"order\").drop(columns=\"order\")\n",
    "\n",
    "print(\"\\n=== FINAL TABLE â€” all models ===\")\n",
    "print(\n",
    "    df_models.to_string(\n",
    "        index=False,\n",
    "        formatters={\n",
    "            \"R2\":   \"{:.3f}\".format,\n",
    "            \"RMSE\": \"{:.3f}\".format,\n",
    "            \"MAE\":  \"{:.3f}\".format,\n",
    "        },\n",
    "    )\n",
    ")\n",
    "\n",
    "# Export CSV pour inclusion dans Overleaf\n",
    "export_path = OUT / \"model_results_final.csv\"\n",
    "df_models.to_csv(export_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Fichier exportÃ© : {export_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 10. DIAGNOSTIC Ã‰CONOMÃ‰TRIQUE (Version Robuste & CorrigÃ©e)\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from scipy.stats import shapiro\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ðŸ” DÃ©marrage de l'analyse des rÃ©sidus sur le modÃ¨le OLS...\")\n",
    "\n",
    "# 1. PrÃ©paration des donnÃ©es pour Statsmodels\n",
    "# On utilise X_nonlin car il contient toutes les features pertinentes (y compris le trend)\n",
    "try:\n",
    "    # On rÃ©cupÃ¨re les noms de colonnes dÃ©finis prÃ©cÃ©demment\n",
    "    cols = nonlin_features\n",
    "except NameError:\n",
    "    # Fallback de sÃ©curitÃ© si la variable n'existe pas\n",
    "    cols = [f\"Var_{i}\" for i in range(X_nonlin.shape[1])]\n",
    "\n",
    "X_sm_df = pd.DataFrame(X_nonlin, columns=cols)\n",
    "X_sm = sm.add_constant(X_sm_df)  # Ajout de la constante (intercept)\n",
    "\n",
    "# 2. Fit du modÃ¨le OLS complet sur tout le dataset\n",
    "model_sm = sm.OLS(y, X_sm).fit()\n",
    "\n",
    "# --- CORRECTION CRITIQUE (Aplatissement des arrays) ---\n",
    "# .values.flatten() garantit que ce sont des tableaux 1D simples\n",
    "# Cela Ã©vite les erreurs \"Multi-dimensional indexing\" avec matplotlib/seaborn\n",
    "residuals = model_sm.resid.values.flatten()\n",
    "y_pred = model_sm.fittedvalues.values.flatten()\n",
    "\n",
    "# --- A. TEST DE NORMALITÃ‰ ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogramme (Version Matplotlib standard, plus robuste ici)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(residuals, bins=20, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Distribution des rÃ©sidus\")\n",
    "plt.xlabel(\"RÃ©sidu\")\n",
    "plt.ylabel(\"FrÃ©quence\")\n",
    "\n",
    "# QQ-Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Qâ€“Q plot of OLS residuals\")\n",
    "plt.tight_layout()\n",
    "save_fig(\"ols_residuals_distribution_qq.png\")\n",
    "\n",
    "# Test de Shapiro-Wilk\n",
    "stat, p_shapiro = shapiro(residuals)\n",
    "print(f\"Test Shapiro-Wilk : p-value = {p_shapiro:.4e}\")\n",
    "if p_shapiro > 0.05:\n",
    "    print(\"âœ”ï¸ H0 acceptÃ©e : Les rÃ©sidus sont distribuÃ©s normalement.\")\n",
    "else:\n",
    "    print(\"âš ï¸ H0 rejetÃ©e : Les rÃ©sidus ne suivent pas une loi normale parfaite.\")\n",
    "\n",
    "# --- B. TEST D'HOMOSCÃ‰DASTICITÃ‰ (Breusch-Pagan) ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_pred, residuals, alpha=0.6, edgecolors='b')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"fitted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals versus fitted values for the OLS model (Homoscedasticity test)\")\n",
    "save_fig(\"ols_residuals_vs_fitted.png\")\n",
    "\n",
    "# Test Breusch-Pagan\n",
    "bp_test = het_breuschpagan(model_sm.resid, model_sm.model.exog)\n",
    "print(f\"Test Breusch-Pagan : p-value = {bp_test[1]:.4e}\")\n",
    "if bp_test[1] > 0.05:\n",
    "    print(\"âœ”ï¸ HomoscÃ©dasticitÃ© confirmÃ©e (Variance constante).\")\n",
    "else:\n",
    "    print(\"âš ï¸ HÃ©tÃ©roscÃ©dasticitÃ© dÃ©tectÃ©e (La variance des erreurs change).\")\n",
    "\n",
    "# --- C. INDÃ‰PENDANCE ET INFLUENCE (Cook's Distance) ---\n",
    "influence = model_sm.get_influence()\n",
    "resid_std = influence.resid_studentized_internal\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(resid_std, marker='o', linestyle='', alpha=0.5)\n",
    "plt.axhline(2, color='red', linestyle='--')\n",
    "plt.axhline(-2, color='red', linestyle='--')\n",
    "plt.title(\"RÃ©sidus StandardisÃ©s (Outliers si > 2)\")\n",
    "\n",
    "# Cook's Distance\n",
    "cooks = influence.cooks_distance[0]\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.stem(np.arange(len(cooks)), cooks, basefmt=\" \")\n",
    "plt.title(\"Cookâ€™s distance for the OLS model\")\n",
    "plt.xlabel(\"Index Observation\")\n",
    "save_fig(\"ols_outliers_cooks_distance.png\")\n",
    "\n",
    "threshold = 4 / len(y)\n",
    "influential_points = np.where(cooks > threshold)[0]\n",
    "print(f\"Nombre d'observations influentes (Cook's D > {threshold:.3f}) : {len(influential_points)}\")\n",
    "\n",
    "# --- Statistical summary ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RÃ‰SUMÃ‰ STATISTIQUE DU MODÃˆLE OLS\")\n",
    "print(\"=\"*40)\n",
    "print(model_sm.summary())\n",
    "summary_path = TAB_DIR / \"ols_summary.txt\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(model_sm.summary().as_text())\n",
    "print(f\"âœ… OLS summary saved: {summary_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_desc = [c for c in df_model.columns if c in [\n",
    "    \"y\",\n",
    "    \"price_elcom_cts\",\n",
    "    \"gdp_per_capita_chf\",\n",
    "    \"motorization_rate_per_1000\",\n",
    "    \"pop_dec31\",\n",
    "    \"summer_temp_c\",\n",
    "    \"winter_temp_c\",\n",
    "    \"co2_emissions_mt\",\n",
    "    \"greens_share\"\n",
    "]]\n",
    "\n",
    "print(\"Variables utilisÃ©es :\", vars_desc)\n",
    "\n",
    "desc_table = (\n",
    "    df_model[vars_desc]\n",
    "    .describe()\n",
    "    .loc[[\"count\", \"mean\", \"std\", \"min\", \"max\"]]\n",
    "    .T\n",
    ")\n",
    "\n",
    "desc_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
